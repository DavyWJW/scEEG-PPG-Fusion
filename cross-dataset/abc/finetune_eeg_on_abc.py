#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MESAé¢„è®­ç»ƒEEGæ¨¡å‹åœ¨ABCä¸Šå¾®è°ƒ

æ”¯æŒå¤šç§å¾®è°ƒç­–ç•¥ï¼š
1. full: å…¨æ¨¡å‹å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰
2. head_only: å†»ç»“ç‰¹å¾æå–å™¨ï¼Œåªå¾®è°ƒåˆ†ç±»å¤´
3. progressive: æ¸è¿›å¼è§£å†»
4. discriminative: å·®å¼‚åŒ–å­¦ä¹ ç‡

ç”¨æ³•:
    python finetune_eeg_on_abc.py \
        --abc_data_dir ../../data/eeg \
        --pretrained_path ./mesa_eeg_model.pth \
        --strategy discriminative \
        --lr 1e-4
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.cuda.amp import autocast, GradScaler
import numpy as np
import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, f1_score, confusion_matrix, classification_report, accuracy_score
from tqdm import tqdm
from datetime import datetime
from collections import Counter, defaultdict
import json
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc
import math
import copy
from copy import deepcopy
import torch.nn.functional as F

warnings.filterwarnings('ignore')


# ============================================================================
# æ¨¡å‹ç»„ä»¶ (AttnSleep 4ç±»)
# ============================================================================

class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)


class SEBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, *, reduction=16):
        super(SEBasicBlock, self).__init__()
        self.conv1 = nn.Conv1d(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm1d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv1d(planes, planes, 1)
        self.bn2 = nn.BatchNorm1d(planes)
        self.se = SELayer(planes, reduction)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.se(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


class GELU(nn.Module):
    def __init__(self):
        super(GELU, self).__init__()

    def forward(self, x):
        return torch.nn.functional.gelu(x)


class MRCNN(nn.Module):
    def __init__(self, afr_reduced_cnn_size):
        super(MRCNN, self).__init__()
        drate = 0.5
        self.GELU = GELU()
        self.features1 = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=50, stride=6, bias=False, padding=24),
            nn.BatchNorm1d(64),
            self.GELU,
            nn.MaxPool1d(kernel_size=8, stride=2, padding=4),
            nn.Dropout(drate),
            nn.Conv1d(64, 128, kernel_size=8, stride=1, bias=False, padding=4),
            nn.BatchNorm1d(128),
            self.GELU,
            nn.Conv1d(128, 128, kernel_size=8, stride=1, bias=False, padding=4),
            nn.BatchNorm1d(128),
            self.GELU,
            nn.MaxPool1d(kernel_size=4, stride=4, padding=2)
        )

        self.features2 = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=400, stride=50, bias=False, padding=200),
            nn.BatchNorm1d(64),
            self.GELU,
            nn.MaxPool1d(kernel_size=4, stride=2, padding=2),
            nn.Dropout(drate),
            nn.Conv1d(64, 128, kernel_size=7, stride=1, bias=False, padding=3),
            nn.BatchNorm1d(128),
            self.GELU,
            nn.Conv1d(128, 128, kernel_size=7, stride=1, bias=False, padding=3),
            nn.BatchNorm1d(128),
            self.GELU,
            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)
        )
        self.dropout = nn.Dropout(drate)
        self.inplanes = 128
        self.AFR = self._make_layer(SEBasicBlock, afr_reduced_cnn_size, 1)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv1d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm1d(planes * block.expansion),
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x1 = self.features1(x)
        x2 = self.features2(x)
        x_concat = torch.cat((x1, x2), dim=2)
        x_concat = self.dropout(x_concat)
        x_concat = self.AFR(x_concat)
        return x_concat


def attention(query, key, value, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn


class CausalConv1d(torch.nn.Conv1d):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):
        self.__padding = (kernel_size - 1) * dilation
        super(CausalConv1d, self).__init__(
            in_channels, out_channels, kernel_size=kernel_size, stride=stride,
            padding=self.__padding, dilation=dilation, groups=groups, bias=bias)

    def forward(self, input):
        result = super(CausalConv1d, self).forward(input)
        if self.__padding != 0:
            return result[:, :, :-self.__padding]
        return result


def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, afr_reduced_cnn_size, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.convs = clones(CausalConv1d(afr_reduced_cnn_size, afr_reduced_cnn_size, kernel_size=7, stride=1), 3)
        self.linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value):
        nbatches = query.size(0)
        query = query.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
        key = self.convs[1](key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
        value = self.convs[2](value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
        x, self.attn = attention(query, key, value, dropout=self.dropout)
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linear(x)


class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2


class SublayerOutput(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerOutput, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))


class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))


class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, afr_reduced_cnn_size, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)
        self.size = size
        self.conv = CausalConv1d(afr_reduced_cnn_size, afr_reduced_cnn_size, kernel_size=7, stride=1, dilation=1)

    def forward(self, x_in):
        query = self.conv(x_in)
        x = self.sublayer_output[0](query, lambda x: self.self_attn(query, x_in, x_in))
        return self.sublayer_output[1](x, self.feed_forward)


class TCE(nn.Module):
    def __init__(self, layer, N):
        super(TCE, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return self.norm(x)


class AttnSleep4Class(nn.Module):
    """4ç±»åˆ«ç‰ˆæœ¬çš„AttnSleepæ¨¡å‹"""

    def __init__(self):
        super(AttnSleep4Class, self).__init__()

        N = 2
        d_model = 80
        d_ff = 120
        h = 5
        dropout = 0.1
        num_classes = 4
        afr_reduced_cnn_size = 30

        self.mrcnn = MRCNN(afr_reduced_cnn_size)

        attn = MultiHeadedAttention(h, d_model, afr_reduced_cnn_size)
        ff = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.tce = TCE(EncoderLayer(d_model, deepcopy(attn), deepcopy(ff), afr_reduced_cnn_size, dropout), N)

        self.fc = nn.Linear(d_model * afr_reduced_cnn_size, num_classes)

    def forward(self, x):
        x_feat = self.mrcnn(x)
        encoded_features = self.tce(x_feat)
        encoded_features = encoded_features.contiguous().view(encoded_features.shape[0], -1)
        final_output = self.fc(encoded_features)
        return final_output


# ============================================================================
# ABC EEGæ•°æ®é›†
# ============================================================================

class ABCEEGDataset(Dataset):
    """ABC EEGæ•°æ®é›† - epochçº§åˆ«"""

    def __init__(self, npz_files, split='train', seed=42, train_ratio=0.6, val_ratio=0.2):
        """
        Args:
            npz_files: NPZæ–‡ä»¶åˆ—è¡¨
            split: 'train', 'val', æˆ– 'test'
            seed: éšæœºç§å­
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
        """
        # ABCæ ‡ç­¾å·²ç»æ˜¯4ç±»ï¼Œæ— éœ€æ˜ å°„
        # 0: Wake, 1: Light, 2: Deep, 3: REM
        self.label_map = {
            0: 0,  # Wake -> Wake
            1: 1,  # Light -> Light
            2: 2,  # Deep -> Deep
            3: 3,  # REM -> REM
        }

        # è·å–æ‰€æœ‰è¢«è¯•ID
        all_subjects = [Path(f).stem for f in npz_files]

        # åˆ’åˆ†æ•°æ®é›†
        train_subjects, temp_subjects = train_test_split(
            all_subjects, test_size=1 - train_ratio, random_state=seed
        )
        val_subjects, test_subjects = train_test_split(
            temp_subjects, test_size=0.5, random_state=seed
        )

        if split == 'train':
            self.subjects = train_subjects
        elif split == 'val':
            self.subjects = val_subjects
        else:
            self.subjects = test_subjects

        # åˆ›å»ºæ–‡ä»¶è·¯å¾„æ˜ å°„
        self.subject_to_file = {Path(f).stem: f for f in npz_files}

        # æ”¶é›†æ‰€æœ‰æ ·æœ¬
        self.samples = []  # (file_path, sample_idx, subject_id)
        class_counts = np.zeros(4, dtype=np.int64)

        print(f"\nåŠ è½½ {split} æ•°æ®é›†...")
        for subj in tqdm(self.subjects, desc=f"åŠ è½½{split}æ•°æ®"):
            if subj not in self.subject_to_file:
                continue

            file_path = self.subject_to_file[subj]
            try:
                data = np.load(file_path)
                y = data['y']

                for idx in range(len(y)):
                    original_label = int(y[idx])
                    if original_label in self.label_map:
                        new_label = self.label_map[original_label]
                        self.samples.append((file_path, idx, subj))
                        class_counts[new_label] += 1

                data.close()
            except Exception as e:
                print(f"  è­¦å‘Š: åŠ è½½ {subj} å¤±è´¥: {e}")
                continue

        self.class_counts = class_counts
        print(f"  {split} set: {len(self.subjects)} è¢«è¯•, {len(self.samples)} æ ·æœ¬")

        class_names = ['Wake', 'Light', 'Deep', 'REM']
        for i, name in enumerate(class_names):
            pct = class_counts[i] / len(self.samples) * 100 if len(self.samples) > 0 else 0
            print(f"    {name}: {class_counts[i]} ({pct:.1f}%)")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        file_path, sample_idx, subject_id = self.samples[index]

        data = np.load(file_path)
        x = data['x'][sample_idx]
        y = data['y'][sample_idx]
        data.close()

        original_label = int(y)
        y_label = self.label_map[original_label]

        x_tensor = torch.from_numpy(x.astype(np.float32)).unsqueeze(0)
        y_tensor = torch.tensor(y_label, dtype=torch.long)

        # Z-scoreæ ‡å‡†åŒ–ï¼ˆæ›´ç¨³å®šçš„ç‰ˆæœ¬ï¼‰
        mean = x_tensor.mean()
        std = x_tensor.std()
        if std > 1e-6:
            x_tensor = (x_tensor - mean) / std
        else:
            x_tensor = x_tensor - mean

        # è£å‰ªæç«¯å€¼
        x_tensor = torch.clamp(x_tensor, -10, 10)

        return x_tensor, y_tensor, subject_id


# ============================================================================
# å¾®è°ƒè®­ç»ƒå™¨
# ============================================================================

class EEGFineTuner:
    """EEGå¾®è°ƒè®­ç»ƒå™¨"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device(f'cuda:{config["gpu_id"]}' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = os.path.join(
            config['output_dir'],
            f"finetune_eeg_{config['strategy']}_{timestamp}"
        )
        os.makedirs(self.output_dir, exist_ok=True)

        # ä¿å­˜é…ç½®
        with open(os.path.join(self.output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        # æ··åˆç²¾åº¦
        self.use_amp = config.get('use_amp', True)
        self.scaler = GradScaler() if self.use_amp else None

    def load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        model = AttnSleep4Class()

        if self.config['pretrained_path'] and os.path.exists(self.config['pretrained_path']):
            print(f"\nåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.config['pretrained_path']}")
            state_dict = torch.load(self.config['pretrained_path'], map_location=self.device)

            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
                state_dict = state_dict['model_state_dict']

            model.load_state_dict(state_dict)
            print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸  æœªæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å¤´è®­ç»ƒ")

        return model.to(self.device)

    def setup_finetune_strategy(self, model):
        """è®¾ç½®å¾®è°ƒç­–ç•¥"""
        strategy = self.config['strategy']

        if strategy == 'full':
            print("\nç­–ç•¥: å…¨æ¨¡å‹å¾®è°ƒ")
            for param in model.parameters():
                param.requires_grad = True

            optimizer = optim.Adam(
                model.parameters(),
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )

        elif strategy == 'head_only':
            print("\nç­–ç•¥: åªå¾®è°ƒåˆ†ç±»å¤´")

            # å†»ç»“ç‰¹å¾æå–å™¨
            for param in model.mrcnn.parameters():
                param.requires_grad = False
            for param in model.tce.parameters():
                param.requires_grad = False

            # åªè®­ç»ƒfcå±‚
            optimizer = optim.Adam(
                model.fc.parameters(),
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )

            trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
            total = sum(p.numel() for p in model.parameters())
            print(f"å¯è®­ç»ƒå‚æ•°: {trainable:,} / {total:,} ({trainable / total * 100:.1f}%)")

        elif strategy == 'progressive':
            print("\nç­–ç•¥: æ¸è¿›å¼è§£å†»")

            for param in model.parameters():
                param.requires_grad = False

            for param in model.fc.parameters():
                param.requires_grad = True

            optimizer = optim.Adam(
                filter(lambda p: p.requires_grad, model.parameters()),
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )

        elif strategy == 'discriminative':
            print("\nç­–ç•¥: å·®å¼‚åŒ–å­¦ä¹ ç‡")

            for param in model.parameters():
                param.requires_grad = True

            base_lr = self.config['learning_rate']
            param_groups = [
                {'params': model.mrcnn.parameters(), 'lr': base_lr * 0.01},
                {'params': model.tce.parameters(), 'lr': base_lr * 0.1},
                {'params': model.fc.parameters(), 'lr': base_lr},
            ]

            optimizer = optim.Adam(param_groups, weight_decay=self.config['weight_decay'])

        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")

        return optimizer

    def unfreeze_layer(self, model, layer_name):
        """è§£å†»æŒ‡å®šå±‚"""
        layer = getattr(model, layer_name, None)
        if layer is not None:
            for param in layer.parameters():
                param.requires_grad = True
            print(f"  è§£å†»: {layer_name}")

    def calculate_class_weights(self, dataset):
        """è®¡ç®—ç±»åˆ«æƒé‡"""
        class_counts = dataset.class_counts
        total = class_counts.sum()

        # è®¡ç®—æƒé‡: æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * è¯¥ç±»æ ·æœ¬æ•°)
        weights = total / (len(class_counts) * class_counts + 1e-6)
        weights = weights / weights.sum() * len(class_counts)  # å½’ä¸€åŒ–

        print(f"\nç±»åˆ«æƒé‡: {weights}")
        return torch.FloatTensor(weights).to(self.device)

    def train_epoch(self, model, dataloader, optimizer, criterion):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        running_loss = 0.0
        total_samples = 0
        nan_count = 0

        for batch_idx, (data, target, _) in enumerate(tqdm(dataloader, desc="Training")):
            data = data.to(self.device)
            target = target.to(self.device)

            # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰nan
            if torch.isnan(data).any():
                nan_count += 1
                continue

            optimizer.zero_grad()

            if self.use_amp:
                with autocast():
                    output = model(data)
                    loss = criterion(output, target)

                # æ£€æŸ¥lossæ˜¯å¦ä¸ºnan
                if torch.isnan(loss) or torch.isinf(loss):
                    nan_count += 1
                    continue

                self.scaler.scale(loss).backward()
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                output = model(data)
                loss = criterion(output, target)

                # æ£€æŸ¥lossæ˜¯å¦ä¸ºnan
                if torch.isnan(loss) or torch.isinf(loss):
                    nan_count += 1
                    continue

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            running_loss += loss.item() * data.size(0)
            total_samples += data.size(0)

            # å®šæœŸæ¸…ç†GPUç¼“å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()

        if nan_count > 0:
            print(f"  è­¦å‘Š: è·³è¿‡äº† {nan_count} ä¸ªå«nançš„batch")

        return running_loss / total_samples if total_samples > 0 else float('nan')

    def evaluate(self, model, dataloader, criterion):
        """è¯„ä¼°"""
        model.eval()
        running_loss = 0.0
        total_samples = 0

        all_preds = []
        all_labels = []
        patient_results = defaultdict(lambda: {'preds': [], 'labels': []})

        with torch.no_grad():
            for data, target, subject_ids in tqdm(dataloader, desc="Evaluating"):
                data = data.to(self.device)
                target = target.to(self.device)

                output = model(data)
                loss = criterion(output, target)

                _, predicted = torch.max(output, 1)

                running_loss += loss.item() * data.size(0)
                total_samples += data.size(0)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(target.cpu().numpy())

                # æŒ‰è¢«è¯•ç»Ÿè®¡
                for i, subj in enumerate(subject_ids):
                    patient_results[subj]['preds'].append(predicted[i].cpu().item())
                    patient_results[subj]['labels'].append(target[i].cpu().item())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        # OverallæŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_preds)
        kappa = cohen_kappa_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='weighted')

        # Per-patientæŒ‡æ ‡
        patient_kappas = []
        patient_accuracies = []
        for subj, data in patient_results.items():
            preds = np.array(data['preds'])
            labels = np.array(data['labels'])

            patient_accuracies.append(accuracy_score(labels, preds))
            if len(np.unique(labels)) > 1:
                patient_kappas.append(cohen_kappa_score(labels, preds))

        median_kappa = np.median(patient_kappas) if patient_kappas else 0
        median_accuracy = np.median(patient_accuracies) if patient_accuracies else 0

        return {
            'loss': running_loss / total_samples,
            'accuracy': accuracy,
            'kappa': kappa,
            'f1': f1,
            'median_kappa': median_kappa,
            'median_accuracy': median_accuracy,
            'all_preds': all_preds,
            'all_labels': all_labels,
            'patient_kappas': patient_kappas
        }

    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 70)
        print("å¼€å§‹EEGå¾®è°ƒè®­ç»ƒ")
        print("=" * 70)

        # åŠ è½½æ•°æ®
        npz_files = sorted(glob.glob(os.path.join(self.config['abc_data_dir'], '*.npz')))
        if len(npz_files) == 0:
            raise FileNotFoundError(f"æœªåœ¨ {self.config['abc_data_dir']} æ‰¾åˆ°NPZæ–‡ä»¶")

        print(f"\næ‰¾åˆ° {len(npz_files)} ä¸ªNPZæ–‡ä»¶")

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = ABCEEGDataset(npz_files, split='train', seed=self.config.get('seed', 42))
        val_dataset = ABCEEGDataset(npz_files, split='val', seed=self.config.get('seed', 42))
        test_dataset = ABCEEGDataset(npz_files, split='test', seed=self.config.get('seed', 42))

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )

        # åŠ è½½æ¨¡å‹
        model = self.load_pretrained_model()

        # è®¾ç½®å¾®è°ƒç­–ç•¥
        optimizer = self.setup_finetune_strategy(model)

        # ç±»åˆ«æƒé‡
        class_weights = self.calculate_class_weights(train_dataset)
        criterion = nn.CrossEntropyLoss(weight=class_weights)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=5, verbose=True
        )

        # è®­ç»ƒå¾ªç¯
        best_kappa = 0
        patience_counter = 0
        best_model_path = os.path.join(self.output_dir, 'best_model.pth')

        history = {
            'train_loss': [],
            'val_loss': [],
            'val_kappa': [],
            'val_accuracy': []
        }

        for epoch in range(1, self.config['num_epochs'] + 1):
            print(f"\nEpoch {epoch}/{self.config['num_epochs']}")
            print("-" * 50)

            # æ¸è¿›å¼è§£å†»
            if self.config['strategy'] == 'progressive':
                if epoch == self.config.get('unfreeze_tce_epoch', 5):
                    print("\nğŸ”“ è§£å†»TCEå±‚")
                    self.unfreeze_layer(model, 'tce')
                    optimizer = optim.Adam(
                        filter(lambda p: p.requires_grad, model.parameters()),
                        lr=self.config['learning_rate'] * 0.1,
                        weight_decay=self.config['weight_decay']
                    )

                elif epoch == self.config.get('unfreeze_all_epoch', 10):
                    print("\nğŸ”“ è§£å†»å…¨éƒ¨å±‚")
                    for param in model.parameters():
                        param.requires_grad = True
                    optimizer = optim.Adam(
                        model.parameters(),
                        lr=self.config['learning_rate'] * 0.01,
                        weight_decay=self.config['weight_decay']
                    )

            # è®­ç»ƒ
            train_loss = self.train_epoch(model, train_loader, optimizer, criterion)

            # éªŒè¯
            val_results = self.evaluate(model, val_loader, criterion)

            # è®°å½•
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_results['loss'])
            history['val_kappa'].append(val_results['kappa'])
            history['val_accuracy'].append(val_results['accuracy'])

            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_results['loss']:.4f}")
            print(f"Val Acc: {val_results['accuracy']:.4f}, Kappa: {val_results['kappa']:.4f}, "
                  f"Median Kappa: {val_results['median_kappa']:.4f}")

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_results['kappa'])

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_results['kappa'] > best_kappa:
                best_kappa = val_results['kappa']
                patience_counter = 0

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_kappa': best_kappa,
                    'config': self.config
                }, best_model_path)

                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Kappa: {best_kappa:.4f})")
            else:
                patience_counter += 1
                if patience_counter >= self.config['patience']:
                    print(f"\nâ¹ï¸  Early stopping at epoch {epoch}")
                    break

        # æµ‹è¯•æœ€ä½³æ¨¡å‹
        print("\n" + "=" * 70)
        print("æµ‹è¯•æœ€ä½³æ¨¡å‹")
        print("=" * 70)

        checkpoint = torch.load(best_model_path)
        model.load_state_dict(checkpoint['model_state_dict'])

        test_results = self.evaluate(model, test_loader, criterion)

        print(f"\næµ‹è¯•ç»“æœ:")
        print(f"  Accuracy: {test_results['accuracy']:.4f}")
        print(f"  Kappa: {test_results['kappa']:.4f}")
        print(f"  Median Kappa: {test_results['median_kappa']:.4f}")
        print(f"  F1: {test_results['f1']:.4f}")

        # åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(
            test_results['all_labels'],
            test_results['all_preds'],
            target_names=['Wake', 'Light', 'Deep', 'REM']
        ))

        # ä¿å­˜ç»“æœ
        results = {
            'strategy': self.config['strategy'],
            'pretrained_path': self.config['pretrained_path'],
            'test_accuracy': float(test_results['accuracy']),
            'test_kappa': float(test_results['kappa']),
            'test_median_kappa': float(test_results['median_kappa']),
            'test_f1': float(test_results['f1']),
            'best_val_kappa': float(best_kappa),
            'history': history
        }

        with open(os.path.join(self.output_dir, 'results.json'), 'w') as f:
            json.dump(results, f, indent=2)

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(test_results['all_labels'], test_results['all_preds'])
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Wake', 'Light', 'Deep', 'REM'],
                    yticklabels=['Wake', 'Light', 'Deep', 'REM'])
        plt.title(f'Confusion Matrix (Kappa: {test_results["kappa"]:.3f})')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.savefig(os.path.join(self.output_dir, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')
        plt.close()

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        axes[0].plot(history['train_loss'], label='Train')
        axes[0].plot(history['val_loss'], label='Val')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].set_title('Loss Curve')

        axes[1].plot(history['val_kappa'], label='Kappa')
        axes[1].plot(history['val_accuracy'], label='Accuracy')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Score')
        axes[1].legend()
        axes[1].set_title('Validation Metrics')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')
        plt.close()

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

        return results


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='å¾®è°ƒMESAé¢„è®­ç»ƒEEGæ¨¡å‹åˆ°ABC')

    # æ•°æ®è·¯å¾„
    parser.add_argument('--abc_data_dir', type=str, required=True,
                        help='ABC EEG NPZæ–‡ä»¶ç›®å½•')
    parser.add_argument('--pretrained_path', type=str, default='',
                        help='MESAé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')

    # å¾®è°ƒç­–ç•¥
    parser.add_argument('--strategy', type=str, default='discriminative',
                        choices=['full', 'head_only', 'progressive', 'discriminative'],
                        help='å¾®è°ƒç­–ç•¥')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--num_epochs', type=int, default=50)
    parser.add_argument('--patience', type=int, default=15)
    parser.add_argument('--weight_decay', type=float, default=1e-5)

    # æ¸è¿›å¼è§£å†»å‚æ•°
    parser.add_argument('--unfreeze_tce_epoch', type=int, default=5)
    parser.add_argument('--unfreeze_all_epoch', type=int, default=10)

    # å…¶ä»–
    parser.add_argument('--output_dir', type=str, default='./finetune_eeg_outputs')
    parser.add_argument('--gpu_id', type=int, default=0)
    parser.add_argument('--num_workers', type=int, default=0)
    parser.add_argument('--no_amp', action='store_true', help='ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--seed', type=int, default=42)

    args = parser.parse_args()

    # æ„å»ºé…ç½®
    config = {
        'abc_data_dir': args.abc_data_dir,
        'pretrained_path': args.pretrained_path,
        'strategy': args.strategy,
        'learning_rate': args.lr,
        'batch_size': args.batch_size,
        'num_epochs': args.num_epochs,
        'patience': args.patience,
        'weight_decay': args.weight_decay,
        'unfreeze_tce_epoch': args.unfreeze_tce_epoch,
        'unfreeze_all_epoch': args.unfreeze_all_epoch,
        'output_dir': args.output_dir,
        'gpu_id': args.gpu_id,
        'num_workers': args.num_workers,
        'use_amp': not args.no_amp,
        'seed': args.seed
    }

    print("\n" + "=" * 70)
    print("MESA â†’ ABC EEG å¾®è°ƒ")
    print("=" * 70)
    print(f"\né…ç½®:")
    print(f"  ç­–ç•¥: {config['strategy']}")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"  é¢„è®­ç»ƒæ¨¡å‹: {config['pretrained_path'] or 'æ— ï¼ˆä»å¤´è®­ç»ƒï¼‰'}")
    print(f"  ABCæ•°æ®ç›®å½•: {config['abc_data_dir']}")
    print(f"  æ··åˆç²¾åº¦: {config['use_amp']}")

    # å¼€å§‹å¾®è°ƒ
    finetuner = EEGFineTuner(config)
    results = finetuner.train()

    print("\n" + "=" * 70)
    print("å¾®è°ƒå®Œæˆ!")
    print("=" * 70)
    print(f"\næœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"  Accuracy: {results['test_accuracy']:.4f}")
    print(f"  Kappa: {results['test_kappa']:.4f}")
    print(f"  Median Kappa: {results['test_median_kappa']:.4f}")


if __name__ == '__main__':
    main()