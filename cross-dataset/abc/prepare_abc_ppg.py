"""
ABCæ•°æ®é›†é¢„å¤„ç†å·¥å…·
ä»NSRRä¸‹è½½çš„EDF/XMLæ–‡ä»¶ â†’ æ ‡å‡†åŒ–HDF5æ ¼å¼ï¼ˆä¸MESAå…¼å®¹ï¼‰

ABCæ•°æ®é›†ç‰¹ç‚¹ï¼š
- 49åå—è¯•è€…ï¼Œæœ€å¤š3ä¸ªæ—¶é—´ç‚¹ï¼ˆbaseline, 9-month, 18-monthï¼‰
- åŒ…å«PPG (Plethé€šé“)ã€EEGã€ECGç­‰ä¿¡å·
- XMLæ ‡æ³¨æ ¼å¼ä¸CFSç±»ä¼¼ï¼ˆNSRRæ ‡å‡†æ ¼å¼ï¼‰
"""

import os
import sys
import numpy as np
import h5py
from pathlib import Path
import warnings
from tqdm import tqdm
from datetime import datetime
import xml.etree.ElementTree as ET

warnings.filterwarnings('ignore')

try:
    import mne
    from scipy import signal
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·è¿è¡Œ: pip install mne scipy")
    sys.exit(1)

print("=" * 80)
print("ABCæ•°æ®é›†é¢„å¤„ç†å·¥å…·")
print("EDF/XML â†’ HDF5 (MESAå…¼å®¹æ ¼å¼)")
print("=" * 80)

# ============================================================================
# é…ç½®éƒ¨åˆ†
# ============================================================================

CONFIG = {
    # ABCæ•°æ®æ ¹ç›®å½•
    'abc_root': "H:/sleepdata/abc",

    # è¾“å‡ºç›®å½•
    'output_dir': "./abc_processed",

    # ç›®æ ‡æ ¼å¼å‚æ•°ï¼ˆä¸MESAä¿æŒä¸€è‡´ï¼‰
    'target_fs': 34.13,  # SleepPPG-Netç›®æ ‡é‡‡æ ·ç‡
    'epoch_length_sec': 30,  # Epoché•¿åº¦(ç§’)
    'samples_per_epoch': 1024,  # æ¯ä¸ªepochçš„é‡‡æ ·ç‚¹æ•° (34.13 * 30 â‰ˆ 1024)
    'target_epochs': 1200,  # ç›®æ ‡epochæ•° (10å°æ—¶)
    'total_samples': 1228800,  # æ€»é‡‡æ ·ç‚¹æ•°

    # æ»¤æ³¢å‚æ•°
    'lowpass_cutoff': 8,  # ä½é€šæ»¤æ³¢æˆªæ­¢é¢‘ç‡ (Hz) - SleepPPG-Netä½¿ç”¨8Hz
    'filter_order': 8,  # æ»¤æ³¢å™¨é˜¶æ•°

    # æ•°æ®è£å‰ª
    'clip_std': 3,  # è£å‰ªåˆ°Â±Nä¸ªæ ‡å‡†å·®

    # å¤„ç†é€‰é¡¹
    'max_files_to_process': None,  # é™åˆ¶å¤„ç†æ•°é‡ (None=å…¨éƒ¨)
    'verbose': True,  # è¯¦ç»†è¾“å‡º

    # ä¿¡å·é€šé“åç§°å˜ä½“
    'ppg_channel_variants': ['Pleth', 'PLETH', 'PPG', 'SpO2', 'Pulse'],
}

# ============================================================================
# ç¡çœ åˆ†æœŸæ˜ å°„
# ============================================================================

# ABCä½¿ç”¨AASMæ ‡å‡†ï¼Œæ˜ å°„åˆ°4ç±»
STAGE_MAPPING = {
    # Wake
    'Wake|0': 0,
    'Wake': 0,
    'W': 0,

    # Light Sleep (N1 + N2)
    'Stage 1 sleep|1': 1,
    'NREM1': 1,
    'N1': 1,
    'Stage 2 sleep|2': 1,
    'NREM2': 1,
    'N2': 1,

    # Deep Sleep (N3)
    'Stage 3 sleep|3': 2,
    'NREM3': 2,
    'N3': 2,
    'Stage 4 sleep|4': 2,  # æ—§çš„R&Kåˆ†æœŸ
    'NREM4': 2,

    # REM
    'REM sleep|5': 3,
    'REM': 3,
    'R': 3,

    # å¿½ç•¥çš„æ ‡ç­¾
    'Movement|6': -1,
    'Movement': -1,
    'Unscored': -1,
    'Unknown': -1,
}


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80 + "\n")


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h}h {m}m {s}s"


# ============================================================================
# æ ‡æ³¨è§£æå™¨
# ============================================================================

class ABCAnnotationParser:
    """è§£æABC XMLæ ‡æ³¨æ–‡ä»¶ï¼ˆNSRRæ ¼å¼ï¼‰"""

    def __init__(self):
        self.label_mapping = STAGE_MAPPING

    def parse_nsrr_xml(self, xml_path):
        """
        è§£æNSRRæ ¼å¼çš„XMLæ ‡æ³¨

        è¿”å›:
            events: ç¡çœ äº‹ä»¶åˆ—è¡¨
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()

            events = []

            # æŸ¥æ‰¾æ‰€æœ‰ScoredEvent
            for event in root.findall('.//ScoredEvent'):
                event_type = event.find('EventType')
                event_concept = event.find('EventConcept')
                start = event.find('Start')
                duration = event.find('Duration')

                # æ£€æŸ¥æ‰€æœ‰å¿…éœ€å…ƒç´ æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ–‡æœ¬å†…å®¹
                if event_type is None or event_concept is None or start is None or duration is None:
                    continue

                # æ£€æŸ¥æ–‡æœ¬å†…å®¹æ˜¯å¦å­˜åœ¨
                if event_type.text is None or event_concept.text is None:
                    continue
                if start.text is None or duration.text is None:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç¡çœ åˆ†æœŸäº‹ä»¶
                if 'Stages' in event_type.text:
                    events.append({
                        'concept': event_concept.text,
                        'start': float(start.text),
                        'duration': float(duration.text)
                    })

            if len(events) == 0:
                return None, f"æœªæ‰¾åˆ°ä»»ä½•ç¡çœ åˆ†æœŸæ ‡æ³¨"

            return events, None

        except ET.ParseError as e:
            return None, f"XMLè§£æé”™è¯¯: {str(e)}"
        except Exception as e:
            return None, f"è§£æå¤±è´¥: {str(e)}"

    def create_epoch_labels(self, events, total_duration_sec, epoch_length=30):
        """
        ä»äº‹ä»¶åˆ›å»ºepochçº§åˆ«çš„æ ‡ç­¾æ•°ç»„

        è¿”å›:
            labels: æ ‡ç­¾æ•°ç»„ [n_epochs]
        """
        n_epochs = int(total_duration_sec // epoch_length)
        labels = np.full(n_epochs, -1, dtype=np.int8)

        for event in events:
            concept = event['concept']
            start_sec = event['start']
            duration_sec = event['duration']

            # æ˜ å°„æ ‡ç­¾
            label = self.label_mapping.get(concept, -1)

            # è®¡ç®—å½±å“çš„epochèŒƒå›´
            start_epoch = int(start_sec // epoch_length)
            end_epoch = int((start_sec + duration_sec) // epoch_length)

            # è®¾ç½®æ ‡ç­¾
            for epoch_idx in range(start_epoch, min(end_epoch + 1, n_epochs)):
                labels[epoch_idx] = label

        return labels


# ============================================================================
# PPGé¢„å¤„ç†å™¨
# ============================================================================

class ABCPPGPreprocessor:
    """ABC PPGä¿¡å·é¢„å¤„ç†ï¼ˆéµå¾ªSleepPPG-Netæ–¹æ³•ï¼‰"""

    def __init__(self, config):
        self.config = config

    def find_channel(self, channel_names, variants):
        """æŸ¥æ‰¾ä¿¡å·é€šé“"""
        for variant in variants:
            for ch in channel_names:
                if variant.upper() in ch.upper():
                    return ch
        return None

    def load_signal(self, edf_path, channel_variants):
        """åŠ è½½æŒ‡å®šé€šé“çš„ä¿¡å·"""
        try:
            raw = mne.io.read_raw_edf(edf_path, preload=False, verbose=False)

            # æŸ¥æ‰¾é€šé“
            channel = self.find_channel(raw.ch_names, channel_variants)

            if channel is None:
                return None, None, f"æœªæ‰¾åˆ°é€šé“: {channel_variants}"

            # åŠ è½½æ•°æ®
            raw.pick_channels([channel])
            raw.load_data()

            data = raw.get_data()[0]
            fs = raw.info['sfreq']

            raw.close()

            return data, fs, None

        except Exception as e:
            return None, None, f"åŠ è½½å¤±è´¥: {str(e)}"

    def preprocess_ppg(self, ppg_signal, original_fs):
        """
        é¢„å¤„ç†PPGä¿¡å·ï¼ˆæŒ‰ç…§SleepPPG-Netæ–¹æ³•ï¼‰

        æ­¥éª¤:
            1. ä½é€šæ»¤æ³¢ï¼ˆ8Hzï¼‰
            2. ä¸‹é‡‡æ ·åˆ°34.13Hz
            3. Clipåˆ°Â±3Ïƒ
            4. Z-scoreæ ‡å‡†åŒ–
            5. å¡«å……/æˆªæ–­åˆ°10å°æ—¶

        è¿”å›:
            processed_ppg: å¤„ç†åçš„PPG [1,228,800]
        """
        target_fs = self.config['target_fs']

        # æ­¥éª¤1: ä½é€šæ»¤æ³¢
        nyq = 0.5 * original_fs
        cutoff = self.config['lowpass_cutoff'] / nyq

        # ç¡®ä¿cutoff < 1
        if cutoff >= 1:
            cutoff = 0.99

        sos = signal.cheby2(
            self.config['filter_order'],
            40,
            cutoff,
            btype='lowpass',
            output='sos'
        )
        filtered_ppg = signal.sosfiltfilt(sos, ppg_signal)

        # æ­¥éª¤2: ä¸‹é‡‡æ ·åˆ°34.13Hz
        downsample_factor = original_fs / target_fs
        n_samples_new = int(len(filtered_ppg) / downsample_factor)
        original_indices = np.arange(len(filtered_ppg))
        new_indices = np.linspace(0, len(filtered_ppg) - 1, n_samples_new)
        downsampled_ppg = np.interp(new_indices, original_indices, filtered_ppg)

        # æ­¥éª¤3: Clipåˆ°Â±3Ïƒ
        mean = np.mean(downsampled_ppg)
        std = np.std(downsampled_ppg)
        clipped_ppg = np.clip(
            downsampled_ppg,
            mean - self.config['clip_std'] * std,
            mean + self.config['clip_std'] * std
        )

        # æ­¥éª¤4: Z-scoreæ ‡å‡†åŒ–
        standardized_ppg = (clipped_ppg - mean) / (std + 1e-8)

        # æ­¥éª¤5: å¡«å……/æˆªæ–­åˆ°10å°æ—¶
        target_samples = self.config['total_samples']
        if len(standardized_ppg) < target_samples:
            pad_length = target_samples - len(standardized_ppg)
            processed_ppg = np.pad(
                standardized_ppg,
                (0, pad_length),
                mode='constant',
                constant_values=0
            )
        else:
            processed_ppg = standardized_ppg[:target_samples]

        return processed_ppg.astype(np.float32)

    def segment_into_windows(self, ppg_signal):
        """
        å°†è¿ç»­ä¿¡å·åˆ†å‰²æˆwindows

        è¿”å›:
            windows: [1200, 1024]
        """
        samples_per_window = self.config['samples_per_epoch']
        n_windows = self.config['target_epochs']

        # ç¡®ä¿ä¿¡å·é•¿åº¦æ­£ç¡®
        expected_length = n_windows * samples_per_window
        if len(ppg_signal) != expected_length:
            # è°ƒæ•´é•¿åº¦
            if len(ppg_signal) < expected_length:
                ppg_signal = np.pad(
                    ppg_signal,
                    (0, expected_length - len(ppg_signal)),
                    mode='constant'
                )
            else:
                ppg_signal = ppg_signal[:expected_length]

        # é‡å¡‘ä¸ºwindows
        windows = ppg_signal.reshape(n_windows, samples_per_window)

        return windows


# ============================================================================
# ABCæ•°æ®é›†å¤„ç†å™¨
# ============================================================================

class ABCDatasetProcessor:
    """ABCæ•°æ®é›†å®Œæ•´å¤„ç†æµç¨‹"""

    def __init__(self, config):
        self.config = config
        self.preprocessor = ABCPPGPreprocessor(config)
        self.parser = ABCAnnotationParser()
        self.results = []

    def find_edf_xml_pairs(self):
        """
        æŸ¥æ‰¾æ‰€æœ‰EDFå’ŒXMLæ–‡ä»¶é…å¯¹

        ABCæ•°æ®é›†ç»“æ„ï¼š
        H:\sleepdata\abc\polysomnography\
        â”œâ”€â”€ edfs\
        â”‚   â”œâ”€â”€ baseline\
        â”‚   â”‚   â””â”€â”€ abc-baseline-900001.edf
        â”‚   â”œâ”€â”€ 9-month\
        â”‚   â””â”€â”€ 18-month\
        â””â”€â”€ annotations-events-nsrr\
            â”œâ”€â”€ baseline\
            â”‚   â””â”€â”€ abc-baseline-900001-nsrr.xml
            â”œâ”€â”€ 9-month\
            â””â”€â”€ 18-month\

        è¿”å›:
            pairs: [{subject_id, visit, edf_path, xml_path, full_id}, ...]
        """
        abc_root = Path(self.config['abc_root'])
        polysomnography_dir = abc_root / "polysomnography"

        pairs = []

        # è®¿é—®ç±»å‹åˆ—è¡¨
        visits = ['baseline']

        for visit in visits:
            # EDFç›®å½•
            edf_dir = polysomnography_dir / "edfs" / visit
            # XMLç›®å½• - æ¯ä¸ªvisitæœ‰å•ç‹¬çš„å­ç›®å½•
            xml_dir = polysomnography_dir / "annotations-events-nsrr" / visit

            if not edf_dir.exists():
                if self.config['verbose']:
                    print(f"âš ï¸ EDFç›®å½•ä¸å­˜åœ¨: {edf_dir}")
                continue

            if not xml_dir.exists():
                if self.config['verbose']:
                    print(f"âš ï¸ XMLç›®å½•ä¸å­˜åœ¨: {xml_dir}")
                continue

            # éå†EDFæ–‡ä»¶
            for edf_file in edf_dir.glob("*.edf"):
                # æ–‡ä»¶å‘½åæ ¼å¼: abc-baseline-900001.edf
                filename = edf_file.stem  # abc-baseline-900001
                parts = filename.split('-')

                if len(parts) >= 3:
                    subject_id = parts[-1]  # 900001
                else:
                    subject_id = filename

                # æŸ¥æ‰¾å¯¹åº”çš„XMLæ–‡ä»¶: abc-baseline-900001-nsrr.xml
                xml_file = xml_dir / f"{filename}-nsrr.xml"

                if xml_file.exists():
                    pairs.append({
                        'subject_id': subject_id,
                        'visit': visit,
                        'edf_path': str(edf_file),
                        'xml_path': str(xml_file),
                        'full_id': f"{subject_id}_{visit}"
                    })
                else:
                    if self.config['verbose']:
                        print(f"âš ï¸ æœªæ‰¾åˆ°XML: {filename}-nsrr.xml")

        return pairs

    def process_single_file(self, pair_info):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        subject_id = pair_info['full_id']
        edf_path = pair_info['edf_path']
        xml_path = pair_info['xml_path']

        try:
            # 1. åŠ è½½PPGä¿¡å·
            ppg_signal, fs, error = self.preprocessor.load_signal(
                edf_path,
                self.config['ppg_channel_variants']
            )

            if error:
                return None, f"PPGåŠ è½½å¤±è´¥: {error}"

            # è·å–åŸå§‹æ—¶é•¿
            duration_sec = len(ppg_signal) / fs

            # 2. è§£ææ ‡æ³¨
            events, error = self.parser.parse_nsrr_xml(xml_path)
            if error:
                return None, f"æ ‡æ³¨è§£æå¤±è´¥: {error}"

            # 3. åˆ›å»ºepochæ ‡ç­¾
            epoch_labels = self.parser.create_epoch_labels(
                events,
                duration_sec,
                self.config['epoch_length_sec']
            )

            # 4. é¢„å¤„ç†PPGä¿¡å·
            processed_ppg = self.preprocessor.preprocess_ppg(ppg_signal, fs)

            # 5. åˆ†å‰²æˆwindows
            ppg_windows = self.preprocessor.segment_into_windows(processed_ppg)

            # 6. å¤„ç†æ ‡ç­¾
            target_epochs = self.config['target_epochs']
            if len(epoch_labels) < target_epochs:
                # å¡«å……ä¸º-1
                final_labels = np.full(target_epochs, -1, dtype=np.int64)
                final_labels[:len(epoch_labels)] = epoch_labels
            else:
                final_labels = epoch_labels[:target_epochs].astype(np.int64)

            # 7. ç»Ÿè®¡
            valid_mask = final_labels >= 0
            n_valid = np.sum(valid_mask)

            label_counts = np.zeros(4, dtype=np.int64)
            for i in range(4):
                label_counts[i] = np.sum(final_labels == i)

            # 8. åˆ›å»ºç»“æœï¼ˆåªä¿ç•™PPGç›¸å…³æ•°æ®ï¼‰
            result = {
                'subject_id': subject_id,
                'ppg': ppg_windows,  # [1200, 1024]
                'labels': final_labels,  # [1200]
                'fs': self.config['target_fs'],
                'n_valid_epochs': int(n_valid),
                'label_distribution': label_counts.tolist(),
                'visit': pair_info['visit']
            }

            return result, None

        except Exception as e:
            import traceback
            return None, f"å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}"

    def save_hdf5(self, result, output_dir):
        """ä¿å­˜ä¸ºHDF5æ ¼å¼ï¼ˆåªä¿ç•™PPGä¿¡å·ï¼Œä¸MESAå…¼å®¹ï¼‰"""
        output_path = Path(output_dir) / f"{result['subject_id']}.h5"

        with h5py.File(output_path, 'w') as f:
            # åªä¿å­˜PPGæ•°æ®
            f.create_dataset(
                'ppg',
                data=result['ppg'],
                compression='gzip',
                compression_opts=4
            )

            # ä¿å­˜æ ‡ç­¾
            f.create_dataset('labels', data=result['labels'])

            # ä¿å­˜å…ƒæ•°æ®
            f.attrs['subject_id'] = result['subject_id']
            f.attrs['fs'] = result['fs']
            f.attrs['n_valid_epochs'] = result['n_valid_epochs']
            f.attrs['visit'] = result['visit']

        return output_path

    def run(self):
        """è¿è¡Œå®Œæ•´å¤„ç†æµç¨‹"""
        print_section("ABCæ•°æ®é›†é¢„å¤„ç†æµç¨‹")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(exist_ok=True, parents=True)

        # 1. æŸ¥æ‰¾æ–‡ä»¶é…å¯¹
        print("ğŸ“ æŸ¥æ‰¾EDF/XMLæ–‡ä»¶é…å¯¹...")
        pairs = self.find_edf_xml_pairs()

        if not pairs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶é…å¯¹!")
            print(f"è¯·æ£€æŸ¥ç›®å½•: {self.config['abc_root']}")
            return

        print(f"âœ… æ‰¾åˆ° {len(pairs)} å¯¹æ–‡ä»¶")

        # æ˜¾ç¤ºç¤ºä¾‹
        if pairs:
            print("\nç¤ºä¾‹:")
            for p in pairs[:3]:
                print(f"  - {p['full_id']}: {Path(p['edf_path']).name}")

        # é™åˆ¶å¤„ç†æ•°é‡
        if self.config['max_files_to_process']:
            pairs = pairs[:self.config['max_files_to_process']]
            print(f"\nâš ï¸ æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {len(pairs)} ä¸ªæ–‡ä»¶")

        # 2. å¤„ç†
        print_section("å¼€å§‹é¢„å¤„ç†")
        print(f"ğŸ“Š å¾…å¤„ç†æ–‡ä»¶: {len(pairs)}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        import time
        start_time = time.time()

        success_count = 0
        failed_count = 0

        for pair_info in tqdm(pairs, desc="é¢„å¤„ç†è¿›åº¦"):
            result, error = self.process_single_file(pair_info)

            if result is not None:
                # ä¿å­˜HDF5
                h5_path = self.save_hdf5(result, output_dir)

                self.results.append({
                    'subject_id': result['subject_id'],
                    'success': True,
                    'h5_path': str(h5_path),
                    'n_valid_epochs': result['n_valid_epochs'],
                    'label_distribution': result['label_distribution'],
                    'visit': result['visit']
                })

                success_count += 1
            else:
                self.results.append({
                    'subject_id': pair_info['full_id'],
                    'success': False,
                    'error': error
                })

                failed_count += 1

                if self.config['verbose']:
                    print(f"\nâŒ {pair_info['full_id']}: {error[:100]}")

        elapsed = time.time() - start_time

        # 3. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(elapsed, success_count, failed_count)

        # 4. ä¿å­˜ç»“æœæ–‡ä»¶
        self.save_results(output_dir)

    def generate_report(self, elapsed_time, success_count, failed_count):
        """ç”ŸæˆæŠ¥å‘Š"""
        print_section("é¢„å¤„ç†å®Œæˆ")

        print(f"â±ï¸ æ€»è€—æ—¶: {format_time(elapsed_time)}")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {elapsed_time / (success_count + failed_count):.2f} ç§’/æ–‡ä»¶")
        print()

        print(f"ğŸ“Š ç»“æœç»Ÿè®¡:")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {failed_count}")
        print(f"   æˆåŠŸç‡: {success_count / (success_count + failed_count) * 100:.1f}%")

        # æˆåŠŸæ–‡ä»¶çš„ç»Ÿè®¡
        if success_count > 0:
            success_results = [r for r in self.results if r['success']]

            total_epochs = sum(r['n_valid_epochs'] for r in success_results)

            # æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
            total_label_counts = np.zeros(4, dtype=np.int64)
            for r in success_results:
                total_label_counts += np.array(r['label_distribution'])

            print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»æœ‰æ•ˆepochæ•°: {total_epochs:,}")

            # æŒ‰visitç»Ÿè®¡
            visits = {}
            for r in success_results:
                v = r['visit']
                if v not in visits:
                    visits[v] = 0
                visits[v] += 1

            print(f"\nğŸ“… è®¿é—®åˆ†å¸ƒ:")
            for v, count in sorted(visits.items()):
                print(f"   {v}: {count} æ–‡ä»¶")

            print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
            label_names = ['Wake', 'Light', 'Deep', 'REM']
            for i, (name, count) in enumerate(zip(label_names, total_label_counts)):
                percentage = count / total_epochs * 100 if total_epochs > 0 else 0
                print(f"   {name:10s}: {count:8,} ({percentage:5.2f}%)")

    def save_results(self, output_dir):
        """ä¿å­˜ç»“æœæ–‡ä»¶"""
        print_section("ä¿å­˜ç»“æœæ–‡ä»¶")

        output_dir = Path(output_dir)
        success_results = [r for r in self.results if r['success']]

        if success_results:
            # 1. è¢«è¯•IDåˆ—è¡¨
            ids_file = output_dir / "processed_subject_ids.txt"
            with open(ids_file, 'w') as f:
                for r in success_results:
                    f.write(r['subject_id'] + '\n')
            print(f"âœ… è¢«è¯•IDåˆ—è¡¨: {ids_file}")

            # 2. HDF5æ–‡ä»¶åˆ—è¡¨
            h5_list_file = output_dir / "h5_file_list.txt"
            with open(h5_list_file, 'w') as f:
                for r in success_results:
                    f.write(r['h5_path'] + '\n')
            print(f"âœ… HDF5æ–‡ä»¶åˆ—è¡¨: {h5_list_file}")

            # 3. è¯¦ç»†ç»Ÿè®¡CSV
            import csv
            stats_file = output_dir / "processing_statistics.csv"
            with open(stats_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'subject_id', 'visit', 'n_valid_epochs',
                    'wake', 'light', 'deep', 'rem'
                ])
                writer.writeheader()
                for r in success_results:
                    writer.writerow({
                        'subject_id': r['subject_id'],
                        'visit': r['visit'],
                        'n_valid_epochs': r['n_valid_epochs'],
                        'wake': r['label_distribution'][0],
                        'light': r['label_distribution'][1],
                        'deep': r['label_distribution'][2],
                        'rem': r['label_distribution'][3],
                    })
            print(f"âœ… è¯¦ç»†ç»Ÿè®¡: {stats_file}")

        print()
        print_section("å…¨éƒ¨å®Œæˆ")
        print(f"ğŸ“ æ‰€æœ‰HDF5æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(success_results)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ¯ å¯ç›´æ¥ç”¨äºfine-tuning!")


# ============================================================================
# åˆå¹¶HDF5æ–‡ä»¶ï¼ˆç”¨äºè®­ç»ƒï¼‰
# ============================================================================

def merge_abc_to_single_hdf5(processed_dir, output_file):
    """
    å°†æ‰€æœ‰å¤„ç†åçš„ABC HDF5æ–‡ä»¶åˆå¹¶ä¸ºå•ä¸ªæ–‡ä»¶ï¼ˆä¸MESAæ ¼å¼å…¼å®¹ï¼‰
    åªä¿ç•™PPGä¿¡å·å’Œæ ‡ç­¾
    åŒæ—¶ç”Ÿæˆsubject_indexæ–‡ä»¶

    è¾“å‡ºæ ¼å¼:
        abc_ppg_with_labels.h5:
            ppg: [total_windows, 1024]
            labels: [total_windows]

        abc_subject_index.h5:
            subjects/{subject_id}/window_indices: ç´¢å¼•æ•°ç»„
            subjects/{subject_id}.attrs: n_windowsç­‰
    """
    processed_dir = Path(processed_dir)
    h5_files = list(processed_dir.glob("*.h5"))

    # æ’é™¤å·²å­˜åœ¨çš„åˆå¹¶æ–‡ä»¶å’Œç´¢å¼•æ–‡ä»¶
    h5_files = [f for f in h5_files if 'abc_ppg_with_labels' not in f.name
                and 'abc_subject_index' not in f.name]

    print(f"ğŸ“ æ‰¾åˆ° {len(h5_files)} ä¸ªHDF5æ–‡ä»¶")

    all_ppg = []
    all_labels = []
    subject_info = []

    current_idx = 0

    for h5_file in tqdm(h5_files, desc="åˆå¹¶æ–‡ä»¶"):
        with h5py.File(h5_file, 'r') as f:
            ppg = f['ppg'][:]  # [1200, 1024]
            labels = f['labels'][:]  # [1200]

            # åªä¿ç•™æœ‰æ•ˆçš„epochs (labels >= 0)
            valid_mask = labels >= 0
            valid_ppg = ppg[valid_mask]
            valid_labels = labels[valid_mask]

            n_windows = len(valid_ppg)

            if n_windows == 0:
                print(f"âš ï¸ è·³è¿‡æ— æœ‰æ•ˆæ•°æ®: {h5_file.name}")
                continue

            all_ppg.append(valid_ppg)
            all_labels.append(valid_labels)

            subject_info.append({
                'subject_id': f.attrs['subject_id'],
                'start_idx': current_idx,
                'n_windows': n_windows,
                'visit': f.attrs.get('visit', 'unknown')
            })

            current_idx += n_windows

    # åˆå¹¶
    all_ppg = np.concatenate(all_ppg, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    print(f"\nğŸ“Š åˆå¹¶ç»“æœ:")
    print(f"   PPGå½¢çŠ¶: {all_ppg.shape}")
    print(f"   æ ‡ç­¾å½¢çŠ¶: {all_labels.shape}")
    print(f"   è¢«è¯•æ•°: {len(subject_info)}")

    # æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    label_names = ['Wake', 'Light', 'Deep', 'REM']
    for i in range(4):
        count = np.sum(all_labels == i)
        pct = count / len(all_labels) * 100
        print(f"   {label_names[i]}: {count:,} ({pct:.1f}%)")

    # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶...")
    with h5py.File(output_file, 'w') as f:
        f.create_dataset('ppg', data=all_ppg, compression='gzip')
        f.create_dataset('labels', data=all_labels)
    print(f"âœ… ä¿å­˜åˆ°: {output_file}")

    # ä¿å­˜subjectç´¢å¼•æ–‡ä»¶ï¼ˆä¸MESAæ ¼å¼å…¼å®¹ï¼‰
    index_file = output_file.parent / "abc_subject_index.h5"
    print(f"\nğŸ’¾ ä¿å­˜è¢«è¯•ç´¢å¼•æ–‡ä»¶...")
    with h5py.File(index_file, 'w') as f:
        subjects_grp = f.create_group('subjects')
        for info in subject_info:
            subj_grp = subjects_grp.create_group(info['subject_id'])
            subj_grp.attrs['n_windows'] = info['n_windows']
            subj_grp.attrs['visit'] = info['visit']
            subj_grp.create_dataset(
                'window_indices',
                data=np.arange(info['start_idx'], info['start_idx'] + info['n_windows'])
            )
    print(f"âœ… ä¿å­˜åˆ°: {index_file}")

    # æ‰“å°è¢«è¯•ç»Ÿè®¡
    print(f"\nğŸ“Š è¢«è¯•ç»Ÿè®¡:")
    visits_count = {}
    for info in subject_info:
        v = info['visit']
        visits_count[v] = visits_count.get(v, 0) + 1
    for v, c in sorted(visits_count.items()):
        print(f"   {v}: {c} è¢«è¯•")

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¢«è¯•éƒ½æœ‰1200ä¸ªwindowsï¼ˆå®Œæ•´10å°æ—¶ï¼‰
    complete_subjects = sum(1 for info in subject_info if info['n_windows'] == 1200)
    print(f"\n   å®Œæ•´è®°å½• (1200 windows): {complete_subjects}/{len(subject_info)}")

    return output_file, index_file


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print(f"\nå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # æ˜¾ç¤ºé…ç½®
    print("é…ç½®ä¿¡æ¯:")
    print(f"  ABCæ ¹ç›®å½•: {CONFIG['abc_root']}")
    print(f"  è¾“å‡ºç›®å½•: {CONFIG['output_dir']}")
    print(f"  ç›®æ ‡é‡‡æ ·ç‡: {CONFIG['target_fs']} Hz")
    print(f"  Epoché•¿åº¦: {CONFIG['epoch_length_sec']}s")
    print(f"  ç›®æ ‡epochs: {CONFIG['target_epochs']}")

    if CONFIG['max_files_to_process']:
        print(f"  âš ï¸ æµ‹è¯•æ¨¡å¼: åªå¤„ç† {CONFIG['max_files_to_process']} ä¸ªæ–‡ä»¶")

    print("\n" + "=" * 80)
    print("ç¡®è®¤å¼€å§‹å¤„ç†? (y/n): ", end='')
    response = input().strip().lower()

    if response != 'y':
        print("\nå·²å–æ¶ˆ")
        return

    # è¿è¡Œé¢„å¤„ç†
    processor = ABCDatasetProcessor(CONFIG)
    processor.run()

    # è¯¢é—®æ˜¯å¦åˆå¹¶
    print("\næ˜¯å¦åˆå¹¶ä¸ºå•ä¸ªHDF5æ–‡ä»¶? (y/n): ", end='')
    response = input().strip().lower()

    if response == 'y':
        output_file = Path(CONFIG['output_dir']) / "abc_ppg_with_labels.h5"
        merge_abc_to_single_hdf5(CONFIG['output_dir'], output_file)

    print("=" * 80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)