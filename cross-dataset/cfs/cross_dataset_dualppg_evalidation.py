#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·¨æ•°æ®é›†éªŒè¯è„šæœ¬ - åŒæµPPGæ¨¡å‹
ç”¨MESAè®­ç»ƒçš„PPG+Unfiltered PPG Cross-Attentionæ¨¡å‹åœ¨CFSæ•°æ®é›†ä¸Šæµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    python cross_dataset_ppg_evaluation.py \
        --model_path ./outputs/ppg_unfiltered_xxx/checkpoints/best_model.pth \
        --cfs_data_dir ../../data
"""

import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast
from sklearn.metrics import (
    classification_report, confusion_matrix,
    accuracy_score, cohen_kappa_score, f1_score
)
import json
import h5py
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import argparse
from datetime import datetime
from collections import defaultdict

warnings.filterwarnings('ignore')

# å¯¼å…¥åŒæµPPGæ¨¡å‹
from ppg_unfiltered_crossattn import PPGUnfilteredCrossAttention


# ============================================================================
# 1. CFS PPGæ•°æ®é›† (ç”¨äºè·¨æ•°æ®é›†æµ‹è¯•)
# ============================================================================

class CFSPPGTestDataset(Dataset):
    """
    CFS PPGæµ‹è¯•æ•°æ®é›†
    åŠ è½½é¢„å¤„ç†å¥½çš„CFS PPGæ•°æ®ç”¨äºè·¨æ•°æ®é›†æµ‹è¯•

    æ•°æ®æ ¼å¼ä¸MESAå¯¹é½:
    - PPG: [1, 1228800] (10å°æ—¶ @ 34.13Hz)
    - Labels: [1200] (1200ä¸ª30ç§’epoch)
    - 4ç±»æ ‡ç­¾: Wake=0, Light=1, Deep=2, REM=3
    """

    def __init__(self, data_path, verbose=True):
        """
        Args:
            data_path: æ•°æ®ç›®å½•ï¼ŒåŒ…å«cfs_ppg_with_labels.h5å’Œcfs_subject_index.h5
        """
        self.data_path = data_path
        self.verbose = verbose

        # æ–‡ä»¶è·¯å¾„
        self.ppg_file = os.path.join(data_path, 'cfs_ppg_with_labels.h5')
        self.index_file = os.path.join(data_path, 'cfs_subject_index.h5')

        # æ£€æŸ¥æ–‡ä»¶
        if not os.path.exists(self.ppg_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°PPGæ–‡ä»¶: {self.ppg_file}")
        if not os.path.exists(self.index_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶: {self.index_file}")

        # å‚æ•°
        self.windows_per_subject = 1200
        self.samples_per_window = 1024

        # åŠ è½½æ‰€æœ‰è¢«è¯•
        self._load_subjects()

    def _load_subjects(self):
        """åŠ è½½æ‰€æœ‰è¢«è¯•ä¿¡æ¯"""
        if self.verbose:
            print(f"ğŸ”§ åŠ è½½CFS PPGæ•°æ®...")
            print(f"   PPGæ–‡ä»¶: {self.ppg_file}")
            print(f"   ç´¢å¼•æ–‡ä»¶: {self.index_file}")

        # è·å–æ‰€æœ‰æœ‰æ•ˆè¢«è¯•
        with h5py.File(self.index_file, 'r') as f:
            all_subjects = list(f['subjects'].keys())

            self.subjects = []
            self.subject_indices = {}

            for subj in all_subjects:
                n_windows = f[f'subjects/{subj}'].attrs['n_windows']
                if n_windows == self.windows_per_subject:
                    indices = f[f'subjects/{subj}/window_indices'][:]
                    self.subjects.append(subj)
                    self.subject_indices[subj] = indices[0]  # èµ·å§‹ç´¢å¼•

        if self.verbose:
            print(f"\nâœ… åŠ è½½å®Œæˆ:")
            print(f"   æœ‰æ•ˆè¢«è¯•æ•°: {len(self.subjects)}")
            print(f"   æ€»epochæ•°: {len(self.subjects) * self.windows_per_subject:,}")

        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        self._compute_label_distribution()

    def _compute_label_distribution(self):
        """è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ"""
        class_counts = np.zeros(4, dtype=np.int64)
        total_valid = 0

        with h5py.File(self.ppg_file, 'r') as f:
            for subj in self.subjects:
                start_idx = self.subject_indices[subj]
                labels = f['labels'][start_idx:start_idx + self.windows_per_subject]

                for label in labels:
                    if 0 <= label < 4:
                        class_counts[label] += 1
                        total_valid += 1

        self.class_counts = class_counts
        self.total_valid_epochs = total_valid

        if self.verbose:
            class_names = ['Wake', 'Light', 'Deep', 'REM']
            print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
            for i, (name, count) in enumerate(zip(class_names, class_counts)):
                pct = count / total_valid * 100 if total_valid > 0 else 0
                print(f"   {i}: {name}: {count:,} ({pct:.1f}%)")

    def __len__(self):
        return len(self.subjects)

    def __getitem__(self, idx):
        """è·å–ä¸€ä¸ªè¢«è¯•çš„å®Œæ•´10å°æ—¶æ•°æ®"""
        subject_id = self.subjects[idx]
        start_idx = self.subject_indices[subject_id]

        with h5py.File(self.ppg_file, 'r') as f:
            # è¯»å–1200ä¸ªçª—å£
            ppg_windows = f['ppg'][start_idx:start_idx + self.windows_per_subject]
            labels = f['labels'][start_idx:start_idx + self.windows_per_subject]

        # æ‹¼æ¥æˆè¿ç»­ä¿¡å·
        ppg_continuous = ppg_windows.reshape(-1)  # [1228800]

        # è½¬æ¢ä¸ºtensor
        ppg_tensor = torch.FloatTensor(ppg_continuous).unsqueeze(0)  # [1, 1228800]
        labels_tensor = torch.LongTensor(labels)  # [1200]

        return ppg_tensor, labels_tensor


# ============================================================================
# 2. è·¨æ•°æ®é›†è¯„ä¼°å‡½æ•°
# ============================================================================

def cross_dataset_ppg_evaluation(model_path, cfs_data_dir, output_dir, config):
    """
    ä½¿ç”¨MESAè®­ç»ƒçš„åŒæµPPGæ¨¡å‹åœ¨CFSæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°
    """

    device = torch.device(f'cuda:{config["gpu_id"]}' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # ========== 1. åŠ è½½MESAè®­ç»ƒçš„æ¨¡å‹ ==========
    print(f"\nğŸ“¦ åŠ è½½MESAè®­ç»ƒçš„åŒæµPPGæ¨¡å‹: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    # åˆ›å»ºæ¨¡å‹
    model = PPGUnfilteredCrossAttention(
        n_classes=4,
        d_model=config.get('d_model', 256),
        n_heads=config.get('n_heads', 8),
        n_fusion_blocks=config.get('n_fusion_blocks', 3),
        noise_config=config.get('noise_config', None)
    ).to(device)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        if 'best_kappa' in checkpoint:
            print(f"   æ¨¡å‹æœ€ä½³éªŒè¯Kappa: {checkpoint['best_kappa']:.4f}")
        if 'epoch' in checkpoint:
            print(f"   è®­ç»ƒepoch: {checkpoint['epoch']}")
    else:
        model.load_state_dict(checkpoint)

    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ========== 2. åŠ è½½CFSæµ‹è¯•æ•°æ® ==========
    print(f"\nğŸ“‚ åŠ è½½CFS PPGæ•°æ®: {cfs_data_dir}")

    test_dataset = CFSPPGTestDataset(cfs_data_dir, verbose=True)
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True
    )

    # ========== 3. åœ¨CFSä¸Šè¿›è¡Œæ¨ç† ==========
    print(f"\nğŸ§ª åœ¨CFSæ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†...")
    print(f"   è¢«è¯•æ•°: {len(test_dataset)}")
    print(f"   Batch size: {config['batch_size']}")

    all_preds = []
    all_labels = []

    # ç”¨äºper-patientè¯„ä¼°
    patient_predictions = defaultdict(list)
    patient_labels = defaultdict(list)

    # è®°å½•æ¨¡æ€æƒé‡
    clean_weights_all = []
    noisy_weights_all = []

    use_amp = config.get('use_amp', True) and torch.cuda.is_available()

    with torch.no_grad():
        for batch_idx, (ppg, labels) in enumerate(tqdm(test_loader, desc="æ¨ç†è¿›åº¦")):
            ppg = ppg.to(device)

            # æ¨ç†
            if use_amp:
                with autocast():
                    outputs = model(ppg)
            else:
                outputs = model(ppg)

            # è·å–æ¨¡æ€æƒé‡
            clean_weight, noisy_weight = model.get_modality_weights()
            if clean_weight is not None:
                clean_weights_all.append(clean_weight.mean().item() if hasattr(clean_weight, 'mean') else clean_weight)
                noisy_weights_all.append(noisy_weight.mean().item() if hasattr(noisy_weight, 'mean') else noisy_weight)

            # å¤„ç†è¾“å‡º
            outputs = outputs.permute(0, 2, 1)  # [B, 1200, 4]

            batch_size = outputs.shape[0]
            for i in range(batch_size):
                patient_idx = batch_idx * config['batch_size'] + i

                # è·å–æœ‰æ•ˆé¢„æµ‹å’Œæ ‡ç­¾
                mask = labels[i] != -1
                if mask.any():
                    valid_outputs = outputs[i][mask]
                    valid_labels = labels[i][mask]

                    _, predicted = valid_outputs.max(1)

                    # ä¿å­˜
                    pred_np = predicted.cpu().numpy()
                    label_np = valid_labels.numpy()

                    patient_predictions[patient_idx].extend(pred_np)
                    patient_labels[patient_idx].extend(label_np)

                    all_preds.extend(pred_np)
                    all_labels.extend(label_np)

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    # ========== 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ==========
    print(f"\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")

    # OverallæŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    kappa = cohen_kappa_score(all_labels, all_preds)
    f1_weighted = f1_score(all_labels, all_preds, average='weighted')
    f1_macro = f1_score(all_labels, all_preds, average='macro')

    # Per-patientæŒ‡æ ‡
    patient_kappas = []
    patient_accuracies = []
    patient_f1s = []

    for patient_idx in patient_predictions:
        if len(patient_predictions[patient_idx]) > 0:
            p_preds = np.array(patient_predictions[patient_idx])
            p_labels = np.array(patient_labels[patient_idx])

            patient_acc = accuracy_score(p_labels, p_preds)
            patient_accuracies.append(patient_acc)

            # åªæœ‰å¤šä¸ªç±»åˆ«æ—¶æ‰è®¡ç®—kappa
            if len(np.unique(p_labels)) > 1:
                patient_kappa = cohen_kappa_score(p_labels, p_preds)
                patient_kappas.append(patient_kappa)

            patient_f1 = f1_score(p_labels, p_preds, average='weighted', zero_division=0)
            patient_f1s.append(patient_f1)

    median_accuracy = np.median(patient_accuracies) if patient_accuracies else 0
    median_kappa = np.median(patient_kappas) if patient_kappas else 0
    median_f1 = np.median(patient_f1s) if patient_f1s else 0

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])

    # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_names = ['Wake', 'Light', 'Deep', 'REM']
    report = classification_report(
        all_labels, all_preds,
        target_names=class_names,
        output_dict=True,
        zero_division=0,
        labels=[0, 1, 2, 3]
    )

    # ========== 5. æ‰“å°ç»“æœ ==========
    print(f"\n{'=' * 70}")
    print(f"è·¨æ•°æ®é›†è¯„ä¼°ç»“æœ: MESA â†’ CFS (åŒæµPPGæ¨¡å‹)")
    print(f"{'=' * 70}")

    print(f"\nğŸ“ˆ OverallæŒ‡æ ‡:")
    print(f"   å‡†ç¡®ç‡: {accuracy * 100:.2f}%")
    print(f"   Kappa: {kappa:.4f}")
    print(f"   F1 (weighted): {f1_weighted:.4f}")
    print(f"   F1 (macro): {f1_macro:.4f}")

    print(f"\nğŸ“ˆ Per-Patient MedianæŒ‡æ ‡:")
    print(f"   å‡†ç¡®ç‡: {median_accuracy * 100:.2f}%")
    print(f"   Kappa: {median_kappa:.4f}")
    print(f"   F1: {median_f1:.4f}")

    if patient_kappas:
        print(f"\n   Kappaåˆ†å¸ƒ:")
        print(f"     Min: {np.min(patient_kappas):.4f}")
        print(f"     25%: {np.percentile(patient_kappas, 25):.4f}")
        print(f"     Median: {median_kappa:.4f}")
        print(f"     75%: {np.percentile(patient_kappas, 75):.4f}")
        print(f"     Max: {np.max(patient_kappas):.4f}")

    # æ¨¡æ€æƒé‡
    if clean_weights_all:
        avg_clean = np.mean(clean_weights_all)
        avg_noisy = np.mean(noisy_weights_all)
        print(f"\nğŸ”€ æ¨¡æ€æƒé‡ (å¹³å‡):")
        print(f"   Clean PPG: {avg_clean:.3f}")
        print(f"   Noisy PPG: {avg_noisy:.3f}")

    print(f"\nğŸ“Š æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½:")
    print(f"{'ç±»åˆ«':<15} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Support':>10}")
    print("-" * 55)
    for name in class_names:
        if name in report:
            p = report[name]['precision']
            r = report[name]['recall']
            f = report[name]['f1-score']
            s = report[name]['support']
            print(f"{name:<15} {p:>10.3f} {r:>10.3f} {f:>10.3f} {int(s):>10}")

    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=class_names,
                                zero_division=0, labels=[0, 1, 2, 3]))

    # ========== 6. å¯è§†åŒ– ==========
    # æ··æ·†çŸ©é˜µ
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names, ax=axes[0])
    axes[0].set_title('Confusion Matrix (Counts)\nMESA â†’ CFS (Dual-Stream PPG)')
    axes[0].set_ylabel('True Label')
    axes[0].set_xlabel('Predicted Label')

    cm_normalized = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-8)
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names, ax=axes[1])
    axes[1].set_title('Confusion Matrix (Normalized)\nMESA â†’ CFS (Dual-Stream PPG)')
    axes[1].set_ylabel('True Label')
    axes[1].set_xlabel('Predicted Label')

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'cross_dataset_ppg_confusion_matrix.png'),
                dpi=150, bbox_inches='tight')
    plt.close()

    # Per-patient Kappaåˆ†å¸ƒ
    if patient_kappas:
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        axes[0].hist(patient_kappas, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
        axes[0].axvline(median_kappa, color='red', linestyle='--', linewidth=2,
                        label=f'Median: {median_kappa:.3f}')
        axes[0].axvline(np.mean(patient_kappas), color='orange', linestyle='--', linewidth=2,
                        label=f'Mean: {np.mean(patient_kappas):.3f}')
        axes[0].set_xlabel('Kappa')
        axes[0].set_ylabel('Number of Patients')
        axes[0].set_title('Per-Patient Kappa Distribution\nMESA â†’ CFS')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Box plot
        axes[1].boxplot(patient_kappas, vert=True)
        axes[1].set_ylabel('Kappa')
        axes[1].set_title('Per-Patient Kappa Box Plot')
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'cross_dataset_ppg_kappa_distribution.png'),
                    dpi=150, bbox_inches='tight')
        plt.close()

    # æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=(10, 6))

    x = np.arange(len(class_names))
    width = 0.25

    precision = [report[name]['precision'] if name in report else 0 for name in class_names]
    recall = [report[name]['recall'] if name in report else 0 for name in class_names]
    f1_scores = [report[name]['f1-score'] if name in report else 0 for name in class_names]

    bars1 = ax.bar(x - width, precision, width, label='Precision', color='steelblue')
    bars2 = ax.bar(x, recall, width, label='Recall', color='coral')
    bars3 = ax.bar(x + width, f1_scores, width, label='F1-Score', color='seagreen')

    ax.set_ylabel('Score')
    ax.set_title('Per-Class Performance: MESA â†’ CFS (Dual-Stream PPG)')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names)
    ax.legend()
    ax.set_ylim([0, 1])
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'cross_dataset_ppg_per_class_performance.png'),
                dpi=150, bbox_inches='tight')
    plt.close()

    # ========== 7. ä¿å­˜ç»“æœ ==========
    results = {
        'experiment': 'Cross-Dataset Evaluation: MESA â†’ CFS (Dual-Stream PPG)',
        'model_path': model_path,
        'cfs_data_dir': cfs_data_dir,
        'n_subjects': len(test_dataset),
        'n_samples': len(all_labels),
        'overall_metrics': {
            'accuracy': float(accuracy),
            'kappa': float(kappa),
            'f1_weighted': float(f1_weighted),
            'f1_macro': float(f1_macro)
        },
        'per_patient_median_metrics': {
            'accuracy': float(median_accuracy),
            'kappa': float(median_kappa),
            'f1': float(median_f1)
        },
        'per_patient_kappa_stats': {
            'min': float(np.min(patient_kappas)) if patient_kappas else 0,
            'max': float(np.max(patient_kappas)) if patient_kappas else 0,
            'mean': float(np.mean(patient_kappas)) if patient_kappas else 0,
            'std': float(np.std(patient_kappas)) if patient_kappas else 0,
            'median': float(median_kappa),
            '25_percentile': float(np.percentile(patient_kappas, 25)) if patient_kappas else 0,
            '75_percentile': float(np.percentile(patient_kappas, 75)) if patient_kappas else 0
        },
        'modality_weights': {
            'clean_ppg': float(np.mean(clean_weights_all)) if clean_weights_all else None,
            'noisy_ppg': float(np.mean(noisy_weights_all)) if noisy_weights_all else None
        },
        'per_class_metrics': {
            name: {
                'precision': float(report[name]['precision']) if name in report else 0,
                'recall': float(report[name]['recall']) if name in report else 0,
                'f1': float(report[name]['f1-score']) if name in report else 0,
                'support': int(report[name]['support']) if name in report else 0
            } for name in class_names
        },
        'confusion_matrix': cm.tolist(),
        'class_distribution': {
            name: int(test_dataset.class_counts[i])
            for i, name in enumerate(class_names)
        },
        'config': config,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    results_path = os.path.join(output_dir, 'cross_dataset_ppg_results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    return results


# ============================================================================
# 3. ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Cross-Dataset Evaluation: MESA â†’ CFS (Dual-Stream PPG)')
    parser.add_argument('--model_path', type=str, required=True,
                        help='MESAè®­ç»ƒçš„åŒæµPPGæ¨¡å‹è·¯å¾„ (.pthæ–‡ä»¶)')
    parser.add_argument('--cfs_data_dir', type=str, required=True,
                        help='CFSæ•°æ®ç›®å½• (åŒ…å«cfs_ppg_with_labels.h5)')
    parser.add_argument('--output_dir', type=str, default='./cross_dataset_ppg_results',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=2,
                        help='Batch size')
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='GPU ID')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½workers')
    parser.add_argument('--no_amp', action='store_true',
                        help='ç¦ç”¨æ··åˆç²¾åº¦')

    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ”„ è·¨æ•°æ®é›†éªŒè¯: MESA â†’ CFS (åŒæµPPGæ¨¡å‹)")
    print("=" * 70)
    print("\néªŒè¯MESAè®­ç»ƒçš„PPG+Unfiltered PPGæ¨¡å‹åœ¨CFSä¸Šçš„æ³›åŒ–èƒ½åŠ›")

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"\nâœ… CUDAå¯ç”¨")
        print(f"   ä½¿ç”¨GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
    else:
        print("\nâš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    # é…ç½®
    config = {
        'batch_size': args.batch_size,
        'gpu_id': args.gpu_id,
        'num_workers': args.num_workers,
        'use_amp': not args.no_amp,
        'd_model': 256,
        'n_heads': 8,
        'n_fusion_blocks': 3,
        'noise_config': {
            'noise_level': 0.1,
            'drift_amplitude': 0.1,
            'drift_frequency': 0.1,
            'spike_probability': 0.01,
            'spike_amplitude': 0.5
        }
    }

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'mesa_to_cfs_ppg_{timestamp}')
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é…ç½®
    with open(os.path.join(output_dir, 'config.json'), 'w') as f:
        json.dump(config, f, indent=2)

    try:
        results = cross_dataset_ppg_evaluation(
            args.model_path,
            args.cfs_data_dir,
            output_dir,
            config
        )

        print(f"\n{'=' * 70}")
        print(f"ğŸ‰ è·¨æ•°æ®é›†è¯„ä¼°å®Œæˆ!")
        print(f"{'=' * 70}")
        print(f"\næœ€ç»ˆç»“æœ (MESAåŒæµPPG â†’ CFS):")
        print(f"   Overallå‡†ç¡®ç‡: {results['overall_metrics']['accuracy'] * 100:.2f}%")
        print(f"   Overall Kappa: {results['overall_metrics']['kappa']:.4f}")
        print(f"   Median Kappa: {results['per_patient_median_metrics']['kappa']:.4f}")
        print(f"   F1 (weighted): {results['overall_metrics']['f1_weighted']:.4f}")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

    print("\n" + "=" * 70)
    print("âœ… å…¨éƒ¨å®Œæˆ!")
    print("=" * 70)


if __name__ == '__main__':
    main()